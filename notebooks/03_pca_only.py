# -*- coding: utf-8 -*-
"""MachineLearningIncrementalCapstone1scaler2PCAXPCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iqUx5nsp15nZ4cHp2xN0bxF3iK5vKM_a

# Machine Learning: Incremental Capstone

https://drive.google.com/drive/folders/1RFvzO7Eyidp4M9GCSO93izHXnaBFfQbF

('FloridaBikeRentals.csv')

Carllos Watts-Nogueira

Due: Jul 12 by 12:59am

# Task 1: Feature engineering
"""

#  Import Libraries - Packages
import pandas as pd
from sklearn.preprocessing import StandardScaler #, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.decomposition import PCA

# Load the data - Inspect Data

# import chardet
# with open('FloridaBikeRentals.csv', 'rb') as f:
#     result = chardet.detect(f.read())
# print(result['encoding'])

# Load the data
df_main = pd.read_csv('FloridaBikeRentals.csv') # after clean Temperature(°C) and Dew point temperature(°C) to Temperature(C) and Dew point temperature(C)

# 1) Analyze the provided dataset and select relevant features
df_main.head()

df_main.info()

# copy
df = df_main.copy(deep=True)

# Change Date from object type to --> datetime
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Change Hour from int64 to --> int8
df['Hour'] = df['Hour'].astype('int8')

# date + hour df.[DateTime]
df['DateTime'] = df['Date'] + pd.to_timedelta(df['Hour'], unit='h')

# drop date, drop hour
df.drop(['Date', 'Hour'], axis=1, inplace=True)

# Recommended to improve performance/memory usage and efficiency?

# int64 to int32
df['Rented Bike Count'] = df['Rented Bike Count'].astype('int32')
df['Visibility (10m)'] = df['Visibility (10m)'].astype('int32')
df['Humidity(%)'] = df['Humidity(%)'].astype('int32')

# float64 to float32
df['Temperature(C)'] = df['Temperature(C)'].astype('float32')
df['Wind speed (m/s)'] = df['Wind speed (m/s)'].astype('float32')
df['Dew point temperature(C)'] = df['Dew point temperature(C)'].astype('float32')
df['Solar Radiation (MJ/m2)'] = df['Solar Radiation (MJ/m2)'].astype('float32')
df['Rainfall(mm)'] = df['Rainfall(mm)'].astype('float32')
df['Snowfall (cm)'] = df['Snowfall (cm)'].astype('float32')

# memory
before = df_main.memory_usage(deep=True).sum()
after = df.memory_usage(deep=True).sum()
print(f"Memory reduced from {before / 1024:.2f} KB to {after / 1024:.2f} KB")

df.describe()

df.duplicated().sum() #= 0

# handle missing values
df.isnull().sum()   #= 0

df['Seasons'].unique()

df['Holiday'].unique()

df['Functioning Day'].unique()

df.shape

df.corr(numeric_only=True)

"""Target = Rented Bike Count

Feacture Correlation analysis

Feature Correlation Analysis
Summary of how each feature correlates with the target (Rented Bike Count):

| Temperature(C) | +0.54 | Strong positive correlation—warmer weather boosts rentals |

| Dew point temperature(C) | +0.38 | Closely tied to temperature, also positively correlated |

| Solar Radiation (MJ/m2) | +0.26 | Sunny conditions encourage biking |

| Visibility (10m) | +0.20 | Clear visibility slightly improves rentals |

| Wind speed (m/s) | +0.12 | Mild wind may be tolerable; extreme wind likely reduces rentals |

| Humidity(%) | –0.20 | High humidity discourages biking |

| Rainfall(mm) | –0.12 | Rain reduces rentals |

| Snowfall (cm) | –0.14 | Snow has a negative impact, though rare in Florida |

"""

# numeric columns
col_numerics = df.select_dtypes(include='number').columns.tolist()
col_numerics

# not numeric columns
col_not_numerics = df.select_dtypes(exclude='number').columns.tolist()
col_not_numerics

# Extract week day (0 = M, 6 = S)
df['Weekday'] = df['DateTime'].dt.weekday

# Flag weekend
df['IsWeekend'] = df['Weekday'].isin([5, 6]).astype(int)

# Extract month
df['Month'] = df['DateTime'].dt.month

# Encode categorical variable
categorical_features = ['Seasons', 'Holiday', 'Functioning Day']
df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)

# Col numeric
numerical_features = ['Temperature(C)', 'Humidity(%)', 'Wind speed (m/s)',
                      'Visibility (10m)', 'Dew point temperature(C)',
                      'Solar Radiation (MJ/m2)', 'Rainfall(mm)',
                      'Snowfall (cm)']

#  Scale the numerical features using StandardScaler
scaler = StandardScaler()
df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])

# 2) Create new features such as:
# Interaction features

# Select numeric predictors only
X_numeric = df_encoded[numerical_features]

# Apply PCA to 2 components (for example)
pca = PCA(n_components=2, random_state=42)
pca_features = pca.fit_transform(X_numeric)

df_encoded['PCA_1'] = pca_features[:, 0]
df_encoded['PCA_2'] = pca_features[:, 1]

# Multiply Temperature by 10 for standardization
# df['Temperature(C)'] = df['Temperature(C)'] * 10
# df

# not need to manually multiply it by 10, applying StandardScaler

# Save the processed dataset as "bike_rental_features.csv"
df_encoded.to_csv('bike_rental_features.csv', index=False)

df_encoded.head()

df_encoded.info()

df_encoded.describe()

df_encoded.corr(numeric_only=True)

"""#  Task 2: Model building"""

# Import libraries
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt

# Define X and y — apenas com PCA
X = df_encoded[['PCA_1', 'PCA_2']]
y = df_encoded['Rented Bike Count']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  2.1) Linear Regression

# Model LinearRegression
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Predictions
y_pred = model_lr.predict(X_test)

print("Linear Regression:")
print("MSE:", mean_squared_error(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))

#  2.2) Ridge Regression (L2 Regularization)

# Instantiate Ridge model with default alpha
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

# Predictions
y_pred_ridge = ridge_model.predict(X_test)

# Evaluation
print("Ridge Regression:")
print("MAE:", round(mean_absolute_error(y_test, y_pred_ridge), 2))
print("MSE:", round(mean_squared_error(y_test, y_pred_ridge), 2))
print("R²:", round(r2_score(y_test, y_pred_ridge), 4))

#  2.3) Lasso Regression (L1 Regularization)

# Train Lasso with default alpha
lasso_model = Lasso(alpha=1.0, max_iter=10000)
lasso_model.fit(X_train, y_train)

# Predict
y_pred_lasso = lasso_model.predict(X_test)

# Evaluate
print("Lasso Regression:")
print("MAE:", round(mean_absolute_error(y_test, y_pred_lasso), 2))
print("MSE:", round(mean_squared_error(y_test, y_pred_lasso), 2))
print("R²:", round(r2_score(y_test, y_pred_lasso), 4))

#  2.4) Elastic Net Regression

# Instantiate ElasticNet with default alpha and l1_ratio
elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000)
elastic_model.fit(X_train, y_train)

# Predictions
y_pred_enet = elastic_model.predict(X_test)

# Evaluation
print("Elastic Net Regression:")
print("MAE:", round(mean_absolute_error(y_test, y_pred_enet), 2))
print("MSE:", round(mean_squared_error(y_test, y_pred_enet), 2))
print("R²:", round(r2_score(y_test, y_pred_enet), 4))

#  2.5) Perform hyperparameter tuning using GridSearchCV

# Define Ridge and the hyperparameter grid
ridge = Ridge()
param_grid = {
    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],
    'fit_intercept': [True, False],
    'solver': ['auto', 'lsqr'],
    'max_iter': [10000, 20000, 50000]
}

# GridSearchCV
grid = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid.fit(X_train, y_train)

# Best model and evaluate
best_ridge = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# Evaluate
y_pred = best_ridge.predict(X_test)
print("Tuned Ridge Regression:")
print("MAE:", round(mean_absolute_error(y_test, y_pred), 2))
print("MSE:", round(mean_squared_error(y_test, y_pred), 2))
print("R²:", round(r2_score(y_test, y_pred), 4))

#  2.6) Evaluate model performance using:
# Mean Absolute Error (MAE)
# Mean Squared Error (MSE)
# R-squared (R²)

#  Consolidate Models

models = {
    'Linear': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet()
}

# param
param_grid = {
    'Ridge': {
        'alpha': [0.1, 1, 10],
        'max_iter': [50000]
    },
    'Lasso': {
        'alpha': [0.001, 0.01, 0.1],
        'max_iter': [50000]
    },
    'ElasticNet': {
        'alpha': [0.1, 1],
        'l1_ratio': [0.2, 0.5, 0.8],
        'max_iter': [50000]
    }
}

results = {}

for name, model in models.items():
    if name in param_grid:
        grid = GridSearchCV(model, param_grid[name], cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
        grid.fit(X_train, y_train)
        best_model = grid.best_estimator_
    else:
        best_model = model.fit(X_train, y_train)

    y_pred = best_model.predict(X_test)

    results[name] = {
        'MAE': mean_absolute_error(y_test, y_pred),
        'MSE': mean_squared_error(y_test, y_pred),
        'R2': r2_score(y_test, y_pred)
    }

# print
print(pd.DataFrame(results).T.round(3))

"""# Task 3: Model building with polynomial features (45 mins)"""

# import libraries
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
import joblib

# Create polynomial features for selected numerical columns
# Train models with polynomial features to capture non-linear relationships
# Compare results with linear models to assess improvements
# Save the best-performing model

poly = PolynomialFeatures(degree=2, include_bias=False)
model = make_pipeline(poly, LinearRegression())

model.fit(X_train, y_train)
y_poly_pred = model.predict(X_test)

# evaluate
print('MAE:', mean_absolute_error(y_test, y_poly_pred))
print('MSE:', mean_squared_error(y_test, y_poly_pred))
print('R²:', r2_score(y_test, y_poly_pred))

# Save
joblib.dump(model, 'best_poly_model.pkl')

# print best_poly_model.pkl, joblib.dump model
print(model)

"""# Task 4: Model evaluation and validation (45 mins)"""

# import libraries
from sklearn.model_selection import cross_val_score
import numpy as np

# Perform cross-validation techniques to validate model performance (on both models- With Polynomial Features and without Polynomial Features)
# Assess models using test data
# Compare results across different regression models

poly_models = {
    'Poly_Linear': make_pipeline(
        StandardScaler(),
        PolynomialFeatures(degree=2, include_bias=False),
        LinearRegression()
    ),
    'Poly_Ridge': make_pipeline(
        StandardScaler(),
        PolynomialFeatures(degree=2, include_bias=False),
        Ridge(alpha=1.0)
    ),
    'Poly_Lasso': make_pipeline(
        StandardScaler(),
        PolynomialFeatures(degree=2, include_bias=False),
        Lasso(alpha=0.1, max_iter=100_000)
    )
}

# Validation
for name, model in poly_models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    print(f"{name} R² CV Mean: {scores.mean():.4f} | Std: {scores.std():.4f}")

# Ridge
ridge = Ridge(alpha=1)
scores = cross_val_score(ridge, X, y, scoring='r2', cv=5)
print('R² mean (cross-val):', np.mean(scores))

# Base models with adjusted max_iter where needed
models = {
    'Linear': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1, max_iter=100000),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=100000)
}

# Polynomial models with scaling
poly_models = {
    'Poly_Linear': make_pipeline(StandardScaler(), PolynomialFeatures(degree=2, include_bias=False), LinearRegression()),
    'Poly_Ridge': make_pipeline(StandardScaler(), PolynomialFeatures(degree=2, include_bias=False), Ridge(alpha=1.0)),
    'Poly_Lasso': make_pipeline(StandardScaler(), PolynomialFeatures(degree=2, include_bias=False), Lasso(alpha=0.1, max_iter=100000))
}

# Evaluation function
def evaluate_model(name, model, X_train, X_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return {
        'MAE': mean_absolute_error(y_test, y_pred),
        'MSE': mean_squared_error(y_test, y_pred),
        'R2': r2_score(y_test, y_pred)
    }

# Run evaluations
results = {}

for name, model in models.items():
    results[name] = evaluate_model(name, model, X_train, X_test)

for name, model in poly_models.items():
    results[name] = evaluate_model(name, model, X_train, X_test)

# Display results
print(pd.DataFrame(results).T.round(3))

"""# Task 5: Reporting and insights (30 mins)"""

# 1.    Summarize findings and key takeaways from the analysis
# 2.    Discuss feature importance and business implications
# 3.    Provide recommendations for further improvements

"""Summary of What I Learned from Modeling Bike Rentals

Performance of the Models

- In this experiment, I first scaled the numerical features using StandardScaler, then applied PCA and kept only the two principal components as inputs for the model. I wanted to see how much information these components could retain without using the original features directly.
The linear models (Linear, Ridge, Lasso) all performed almost identically, with R² around 0.481, MAE close to 342, and MSE near 216,000. This tells me that just using two PCA components captures some patterns, but not enough for strong predictions.
- Once I tried Polynomial models (Poly_Linear, Poly_Ridge, Poly_Lasso), performance improved significantly — R² jumped to ~0.617, and MAE dropped to around 279. That shows the importance of non-linear modeling, even when working with reduced data. It made a noticeable difference.

Making the Models More Stable

- Polynomial features increased complexity, and I learned they can cause convergence problems if the training setup isn’t handled properly. So I increased max_iter and added regularization with Ridge and Lasso to help stabilize the training. These adjustments made the models more reliable and helped prevent overfitting, even though the data was limited to just PCA features.

Business Implications:

- Even though the results weren’t perfect, this approach still offers some value. With decent accuracy, a bike rental company could start forecasting rental counts using just compressed data (like PCA). It may help in systems - - where storing many variables isn’t practical or data is anonymized.
- However, the predictions are not accurate enough yet to drive high-stakes decisions like pricing or fleet rebalancing. Still, it’s a good starting point for situations where computational simplicity or privacy is important.

Recommendations for Further Improvements

If I do this again, I’d take a few steps to improve:
- Instead of using only two PCA components, I’d try keeping more — maybe 5 or 10 — to preserve more variance.
- I’d consider using tree-based models like RandomForest or XGBoost, which don’t require scaling and might pick up on deeper patterns.
- Also, I realized that dropping the original features might’ve hurt model performance. So I’d compare results between using PCA-only versus using PCA plus selected original features.
- Finally, adding visualization like residual plots or prediction vs. actual charts would help me understand where the models struggle.
"""

# plot L Regression modal
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Linear Regression: Actual vs Predicted")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)  # Reference line
plt.grid(True)
plt.show()

# Plot Ridge Regression
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred_ridge, alpha=0.6, color='green')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Ridge Regression: Actual vs Predicted")
plt.grid(True)
plt.show()

# Plot Lasso Regression
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred_lasso, alpha=0.6, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Lasso Regression: Actual vs Predicted")
plt.grid(True)
plt.show()

# Plot Elastic Net Regrression
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred_enet, alpha=0.6, color='darkcyan')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Elastic Net: Actual vs Predicted")
plt.grid(True)
plt.show()

# Plot Perfom hyperparameter tuning using GridSearchCV
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='navy')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Tuned Ridge: Actual vs Predicted")
plt.grid(True)
plt.show()

# Plot Poly_Linear
model = poly_models['Poly_Linear']
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Poly_Linear: Actual vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Poly Ridge
model = poly_models['Poly_Ridge']
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='green')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Poly_Ridge: Actual vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Poly Lasso
model = poly_models['Poly_Lasso']
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='purple')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Poly_Lasso: Actual vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.show()